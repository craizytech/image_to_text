Document Extraction with LLMs
This project demonstrates how to extract structured information from a PDF document using a pre-trained language model (MiniCPM-Llama3-V-2_5) with PyTorch, PyMuPDF, and the Python Imaging Library (PIL). The code extracts text data such as questions, lists, and tables from PDF images, processing them as markdown tables where applicable.

Prerequisites
To run this code, you need to have the following libraries installed:

torch
PIL (Python Imaging Library)
transformers
fitz (PyMuPDF)
To install these dependencies, you can use pip:

bash
Copy code
pip install torch transformers pillow pymupdf
Code Overview
Model Initialization: The code loads a pre-trained language model, MiniCPM-Llama3-V-2_5, using the AutoModel and AutoTokenizer classes from the transformers library. The model is set to use a half-precision floating point (torch.float16) for efficiency and runs on a CUDA-enabled GPU.

python
Copy code
model = AutoModel.from_pretrained(
    "openbmb/MiniCPM-Llama3-V-2_5", trust_remote_code=True, torch_dtype=torch.float16
)
model = model.to(device="cuda")

tokenizer = AutoTokenizer.from_pretrained(
    "openbmb/MiniCPM-Llama3-V-2_5", trust_remote_code=True
)
model.eval()
PDF Processing: The script loads a PDF document using PyMuPDF (fitz), iterates through each page, and converts it into images. These images are stored in a list for further processing.

python
Copy code
pdf_path = "2021.pdf"
pdf_document = fitz.open(pdf_path)

images = []

for page_number in range(len(pdf_document)):
    page = pdf_document.load_page(page_number)
    pix = page.get_pixmap()
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    images.append(img)

pdf_document.close()
Text Extraction: A predefined question template is used to instruct the model on what to extract from the images, such as questions, tables, and lists. The response is generated by passing the first image and the question to the model's chat method. Note that in a real implementation, you should loop through all images in the images list to process the entire document.

python
Copy code
question = """Extract all the questions as text in this image.
if there are no questions skip it
If there is a header or a footer, or a front page, just ignore it.
Extract tables as markdown tables if there are any.
Don't use the subtitles for the list items, just return the list as text.
if there is something that you cannot record as text add description
"""
msgs = [{"question": "question", "content": question}]

res = model.chat(
    image=images[0], # loop through all images in images list here
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.7,
)
print(res)
How to Use
Download the Model: Make sure you have access to the MiniCPM-Llama3-V-2_5 model. You may need appropriate permissions or API tokens to load the model if it's hosted remotely.

Prepare the PDF: Place the PDF you want to extract information from in the same directory as the script or provide the appropriate path to the pdf_path variable.

Run the Script: Execute the script with Python:

bash
Copy code
python script_name.py
Output: The extracted content will be printed to the console. Modify the script as needed to handle multiple pages and images in the PDF.

Additional Information
For more detailed information on document extraction with language models, visit this blog post.

Limitations
This script processes only the first page of the PDF. To extract information from all pages, modify the code to loop through all images in the images list.
The model's performance depends on the quality and clarity of the PDF content. Results may vary based on the complexity of the documents.
Contributing
Feel free to fork this repository and submit pull requests with improvements or additional features!

This README provides a clear guide on what the code does, how to set it up, and where to find more information. Let me know if you need further modifications!